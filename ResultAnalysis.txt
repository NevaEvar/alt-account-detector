16.7.2025
Trained the model with short one off message pairs. The messages were vectorized and cosine was used to find features. 
Data was should've been fairly balanced for both true and false and the data set was fairly small.
TF + IDF in combination with random forest was used. (22.7.2025 correction/addition the properties of random forest weren't utilized until later date that will be stated in the notes.) 

Results:
              precision    recall  f1-score   support

           0       0.58      0.17      0.27       183
           1       0.53      0.88      0.66       195

    accuracy                           0.54       378
   macro avg       0.56      0.53      0.47       378
weighted avg       0.56      0.54      0.47       378

Improvements for the future:
Extract some other feature from the vectors to account for different features? 

Instead of training the model on small phrases add up messages from same user into bigger texts. Rational: Same user usually has common phrases that they use often. Thus it would stand to reason that including more text would bring these phrases more visible for the model.

Balance the data more. Rational: Recall on "Is message from the same user? yes." is very good while the opposite is very bad. 



17.7.2025
Checking balance of the data set revealed that the training data has more cases for True then False. Thus balanced the dataset better this time.
Trained the model with longer message pairs and extracted them from 3 raw text files meaning 3 times the training data compared to the first one.
Cosine and absolude difference were used this time for the vector features. (22.7.2025 correction absolute difference was not given a unique feature rather it was combined to cosine thus not utilizing the random forest at all)
As seen below there was a significant increase in both precision as well as accuracy.

Results:
              precision    recall  f1-score   support

           0       0.60      0.66      0.63        53
           1       0.62      0.57      0.59        53

    accuracy                           0.61       106
   macro avg       0.61      0.61      0.61       106
weighted avg       0.61      0.61      0.61       106

Improvements for the future:
Increasing the training data even more. Rational: Since the increase in training data raised both precision and recall it wouldn't hurt to increase it and see what happens.



18.7.2025
Trained with the same longer message pair set but doubled the amount of training data by extracting messages from 6 raw text files instead. 
Since there is a noticable decrease in percision and recall some form of overfitting might be happening.

Results:
              precision    recall  f1-score   support

           0       0.58      0.59      0.58       104
           1       0.58      0.57      0.57       104

    accuracy                           0.58       208
   macro avg       0.58      0.58      0.58       208
weighted avg       0.58      0.58      0.58       208

Improvements for the future:
Better feature engineering. Rational: There's still some wiggle room there.
Decreasing data set size. Rational: Reduces potential overfitting even if random forest is more resiliant to it.



22.7.2025
Realized a massive issue with the training data building process for False cases. Now fixed to actually contain opposing message pairs. This fix also increased the precision and recall significantly. Also removing the absolute difference feature contributed to the performance positively. 
Current model has 60 messages per row with an even 50/50 split for message1 and -2. 

Results:
              precision    recall  f1-score   support

           0       0.68      0.72      0.70       104
           1       0.70      0.66      0.68       104

    accuracy                           0.69       208
   macro avg       0.69      0.69      0.69       208
weighted avg       0.69      0.69      0.69       208

Improvements for the future:
Better feature engineering. Rational: Didn't improve it for the current version yet.
Increasing data set size. Rational: Increasing the data size only improved the perfomance on the current version. Testing further wouldn't hurt.



30.7.2025
Added more features including punctuation frequency and average word length. The model now is working with 3 features and the performance is significantly improved yet again. The training data size is the same as before but will be later increased to test if there're more gains to make there.
Additional improvements are that the model is more balansed on precision and recall for both true and false cases. 

Results: 
              precision    recall  f1-score   support

           0       0.80      0.79      0.79       104
           1       0.79      0.80      0.79       104

    accuracy                           0.79       208
   macro avg       0.79      0.79      0.79       208
weighted avg       0.79      0.79      0.79       208


Improvements for the future: 
Increasing data set size. Rational: discussed above in previouse update.
Adding more features. Rational: same as before.
More comprehensive testing. Rational: To make sure that the model is actually performing as well as the results are showing. 



31.7.2025
This section contains take aways from a feedback session and general planing for moving to an actual implemented version of this model. 

Main implemented version challenge will be building the TF-IDF matrix from the targeted servers corpus. Since the matrises vectors will have more or less unit vectors the prediction out come might not be as accurate as in the test cases. To make sure that transitioning into an actually implemented version is smooth the training data needs to be more general to account for more variations.

Potential features brought up in the discussion:
-Vibe of a user. Current implementation: Vibe is built off of topics, word usage/typing style that are addressed at least partially by the CosineDiff.
-Repeated gif use. For example nieche gifs. Current implementation: Not done and is harder to do since the data set doesn't contain gifs.
-Weird acronyms. Current implementation: Is accounted for by the CosineDiff.
-Minimum time gatekeeping. (when user leaves and joins the server). Current implementation: Is reliant on meta data that isn't in the training data but could be explored further since it's a dead give away.
-Replying to specific users. Current implementation: Is some what accounted for by CosineDiff. 
-Style of writing. Current implementation: Is currently accounted for by average word length, average punctuation rate and CosineDiff.
-Topics. Current implementation: Is somewhat accounted for by the CosineDiff but could be built into it's own feature. 
-Channel usage. Current implementation: Non existant since the data set has no meta data. But could be a very good give a way as a feature.

Potential problems: 
-Training data not general enough so it doesn't cover enough writing styles or servers.
-Better accuracy with people who have more messages typed. (Could be addressed by making the training data rely on less message samples but will decrease performance.

Next stage of development:
Seeing how the model performs when pulling messages and testing it on actual servers. And after that analyzing results and trying to find areas of improvement.



11.8.2025
Made a trial run on using the model on a smaller discord server where 5 000 messages were scraped and 14 total columns made. Main goal was to test how the model performs in an actual use case.
Key differences to consider while reviewing results. The cosine difference was effected due to corpus being smaller (built from compleatly different messages) and thus creating different TF-IDF vectors.

Results:
              precision    recall  f1-score   support

           0       1.00      0.29      0.44         7
           1       0.58      1.00      0.74         7

    accuracy                           0.64        14
   macro avg       0.79      0.64      0.59        14
weighted avg       0.79      0.64      0.59        14

Confusion Matrix:
[2 5] [TN FP]
[0 7] [FN TP]

Tested on model with results of:
              precision    recall  f1-score   support

           0       0.80      0.79      0.79       104
           1       0.79      0.80      0.79       104

    accuracy                           0.79       208
   macro avg       0.79      0.79      0.79       208
weighted avg       0.79      0.79      0.79       208


Improvements for future: 
Address the issue with the different corpus size/context.
Decreasing model training size. Rational: The model would be more generalized.



16.8.2025
Made the model with a smaller dataset to see if it would make better predictions on active servers messages. The accuracy is better and the model is less biased on guessing true (from same person). Issue still remains that the negative cases recall is still just a coin flip.

Results: 
              precision    recall  f1-score   support

           0       0.80      0.57      0.67         7
           1       0.67      0.86      0.75         7

    accuracy                           0.71        14
   macro avg       0.73      0.71      0.71        14
weighted avg       0.73      0.71      0.71        14

Confusion Matrix:
[4 3] [TN FP]
[1 6] [FN TP]

Tested on model with results of:
              precision    recall  f1-score   support

           0       0.67      0.81      0.73        27
           1       0.76      0.59      0.67        27

    accuracy                           0.70        54
   macro avg       0.71      0.70      0.70        54
weighted avg       0.71      0.70      0.70        54


Improvements:
Making sure the results won't improve by increasing or decreasing the model training dataset. Approach this by testing multiple different server channels and message pairs to different models that have an increasing amount of training data. (preferabliy even different servers). Rational: Finding an approximate generalization point so that other factors can be evaluated. 
Adding more variance to the training data. Meaning collecting message pairs from different servers more. Rational: Will generalize the data more.
